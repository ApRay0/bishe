{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53a7a0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so, 6): Symbol not found: __ZN2at8internal13_parallel_runExxxRKNSt3__18functionIFvxxmEEE\n  Referenced from: /usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so\n  Expected in: /usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch/lib/libtorch_cpu.dylib\n in /usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4y/hct1z7_52kq3gj_fnlhkmktc0000gn/T/ipykernel_4957/4171871758.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_configuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_graph_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mformat_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraphReconstructor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraphPointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBartTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/WorkSpace/bishe/module.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn_conv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgcn_conv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRGCNConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaForMaskedLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhetero_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m from torch_geometric.data.storage import (BaseStorage, EdgeStorage,\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda_spec\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcpu_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         raise ImportError(f\"Could not find module '{library}_cpu' in \"\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/bishe/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so, 6): Symbol not found: __ZN2at8internal13_parallel_runExxxRKNSt3__18functionIFvxxmEEE\n  Referenced from: /usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so\n  Expected in: /usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch/lib/libtorch_cpu.dylib\n in /usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch import nn\n",
    "from logging import getLogger\n",
    "from data import Vocab, NLP, S2SDataset\n",
    "from utils import build_optimizer, init_seed, init_logger, init_device, read_configuration, collate_fn_graph_text, \\\n",
    "    format_time\n",
    "from module import GraphEncoder, GraphReconstructor, GraphPointer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BertModel, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config):\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "\n",
    "    logger.info(config)\n",
    "    init_seed(config[\"seed\"], config[\"reproducibility\"])\n",
    "    device = init_device(config)\n",
    "\n",
    "    logger.info(\"Build node and relation vocabularies.\")\n",
    "    vocabs = dict()\n",
    "    vocabs[\"node\"] = Vocab(config[\"node_vocab\"])\n",
    "    vocabs[\"relation\"] = Vocab(config[\"relation_vocab\"])\n",
    "\n",
    "    # logger.info(\"Build Teacher Model.\")\n",
    "    # teacher = BartForConditionalGeneration.from_pretrained(config[\"teacher_dir\"])\n",
    "    # teacher.requires_grad = False\n",
    "    # for para in teacher.parameters():\n",
    "    #     para.requires_grad = False\n",
    "    # teacher.to(device)\n",
    "\n",
    "    logger.info(\"Build Student Model.\")\n",
    "    student = GraphEncoder(vocabs[\"node\"].size(), vocabs[\"relation\"].size(),\n",
    "                           config[\"gnn_layers\"], config[\"embedding_size\"], config[\"node_embedding\"])\n",
    "    student.load_state_dict(torch.load(config[\"external_model\"])[\"student\"])\n",
    "    student.to(device)\n",
    "\n",
    "    logger.info(\"Build PLM Model.\")\n",
    "    bart_tokenizer = BartTokenizer.from_pretrained(config[\"fine_tuned_plm_dir\"])\n",
    "    plm = BartForConditionalGeneration.from_pretrained(config[\"fine_tuned_plm_dir\"])\n",
    "    plm.to(device)\n",
    "\n",
    "    logger.info(\"Create testing dataset.\")\n",
    "    test_dataloader = DataLoader(\n",
    "        S2SDataset(data_dir=config[\"data_dir\"], dataset=config[\"dataset\"],\n",
    "                   tokenizer=bart_tokenizer, node_vocab=vocabs[\"node\"], relation_vocab=vocabs[\"relation\"],\n",
    "                   num_samples=\"all\", usage=\"test\"),\n",
    "        batch_size=config[\"test_batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        drop_last=False,\n",
    "        collate_fn=collate_fn_graph_text,\n",
    "        pin_memory=True)\n",
    "\n",
    "    student.eval()\n",
    "    # teacher.eval()\n",
    "    plm.eval()\n",
    "    idx = 0\n",
    "    generated_text = []\n",
    "    reference_text = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            nodes, edges, types, node_masks, kd_description, kd_description_masks, kd_positions, \\\n",
    "                recon_relations, recon_positions, recon_masks, gen_outputs, gen_masks, pointer, pointer_masks = batch\n",
    "\n",
    "            # kd_description = kd_description.to(device)\n",
    "            # kd_description_masks = kd_description_masks.to(device)\n",
    "            # output_dict = teacher(kd_description,\n",
    "            #                       attention_mask=kd_description_masks,\n",
    "            #                       output_hidden_states=True,\n",
    "            #                       return_dict=True)\n",
    "            # positions = kd_positions.unsqueeze(-1).expand(-1, -1, output_dict[\"encoder_last_hidden_state\"].size(-1)).to(device)\n",
    "            # teacher_embeddings = torch.gather(output_dict[\"encoder_last_hidden_state\"], dim=1, index=positions).detach()\n",
    "\n",
    "            nodes = nodes.to(device)\n",
    "            student_embeddings = student(nodes, edges, types)\n",
    "\n",
    "            node_masks = node_masks.to(device)\n",
    "            generated_ids = plm.generate(input_ids=None,\n",
    "                                         inputs_embeds=student_embeddings,\n",
    "                                         attention_mask=node_masks,\n",
    "                                         num_beams=4,\n",
    "                                         max_length=config[\"max_seq_length\"],\n",
    "                                         early_stopping=True)\n",
    "\n",
    "            generated = bart_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            reference = bart_tokenizer.batch_decode(gen_outputs, skip_special_tokens=True)\n",
    "            generated_text.extend(generated)\n",
    "            reference_text.extend(reference)\n",
    "\n",
    "            idx += 1\n",
    "            logger.info(\"Finish {}-th example.\".format(idx))\n",
    "\n",
    "    assert len(generated_text) == len(reference_text)\n",
    "    saved_file = \"{}-{}.res\".format(config[\"dataset\"], config[\"num_samples\"])\n",
    "    saved_file_path = os.path.join(config[\"output_dir\"], saved_file)\n",
    "    fout = open(saved_file_path, \"w\")\n",
    "    for i in range(len(generated_text)):\n",
    "        fout.write(\"Generated text: \" + generated_text[i].strip() + \"\\n\")\n",
    "        fout.write(\"Reference text: \" + reference_text[i].strip() + \"\\n\")\n",
    "    fout.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
