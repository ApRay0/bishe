{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca07f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "<br/>\n",
    "<br/>\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "       欢迎大家使用Notebook（Jupyter）计算平台，Notebook（Jupyter）计算平台是数据平台部为数据科学、数据分析、数据工程相关的研发、产品、运营同事提供的科学计算平台。 大家可以通过下面的索引或者左侧的文档导航快速了解Notebook计算平台，针对具体问题大家也可以使用右上方的搜索功能快速定位相关的帮助文档。\n",
    "\n",
    "**新人请联系陆军建(lujunjian)、闻永萱(wenyongxuan)、王爽爽(wangshuangshuang)入群『大数据：Jupyter沟通群』，系统升级及重要公告以群通知为准**\n",
    "\n",
    "一、 平台介绍\n",
    "106. [快速了解Notebook（Jupyter）计算平台](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=64)\n",
    "\n",
    "\n",
    "104 [Notebook平台使用说明](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=104)\n",
    "\n",
    "      从实例申请，到熟悉开发环境，新手引导及demo，如何进行扩展包安装，如何查询数据、处理文件、处理数据、进行数据分析、可视化、建模、报告制作及分享、以及如何进行协作开发（支持项目管理、版本管理），使用说明中均配套提供了notebook demo，方便大家查看、克隆（可以基于提供的模板进行二次开发）：\n",
    "\n",
    "107 [实例申请](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=65)\n",
    "108 [熟悉开发环境](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=66)\n",
    "109 [新手引导及demo示例](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=67)\n",
    "110 [各kernel简介](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=68)\n",
    "111 [python、R、pyspark第三方库安装](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=69)\n",
    "112 [代码块](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=70)\n",
    "113 [数据查询及示例](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=71)\n",
    "114 [文件的上传和下载](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=87)\n",
    "115 [数据处理及示例](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=88)\n",
    "118 [数据分析及示例](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=72)\n",
    "119 [数据可视化及示例](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=73)\n",
    "120 [数据建模及示例](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=74)\n",
    "121 [数据报告的编写和分享](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=75)\n",
    "122 [使用Notebook进行协作开发](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=76)\n",
    "123 [通过conda创建新kernel 、安装包](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=78)\n",
    "124 [代码自动补全和函数帮助文档](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=77)\n",
    "\n",
    "105 [Notebook平台专题介绍](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=63)\n",
    "    \n",
    "    针对进阶使用，我们提供了一些专题供大家参考：\n",
    "125 [pyspark进行大数据量处理](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=79)\n",
    "126 [实验数据离线分析](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=82)\n",
    "127 [因果推断与实验检验(DID工具包使用介绍)](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=83)\n",
    "128 [通过pyXlearning执行xgb、tf等作业](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=94)\n",
    "129 [TensorFlow扩展](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=84)\n",
    "130 [调度功能](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=85)\n",
    "131 [用户数据备份](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=86)\n",
    "132 [OLAP查询(KwaiSQL)](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=90)\n",
    "133 [python发送邮件](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=91)\n",
    "\n",
    "四、常见问题\n",
    "134 [实例资源、监控使用及问题处理方案](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=80)\n",
    "135 [断网会导致Jupyter任务中断吗](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=95)\n",
    "136 [常用扩展包问题](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=96)\n",
    "137 [Python版本的说明](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=92)\n",
    "138 [jupyter常见问题整理](https://notebook-community.test.gifshow.com/#/project/detail/53/file?fileDetailId=97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95e48b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so, 6): Symbol not found: __ZN2at8internal13_parallel_runExxxRKNSt3__18functionIFvxxmEEE\n  Referenced from: /usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so\n  Expected in: /usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch/lib/libtorch_cpu.dylib\n in /usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4y/hct1z7_52kq3gj_fnlhkmktc0000gn/T/ipykernel_78921/3367934458.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_configuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_graph_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mformat_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraphReconstructor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraphPointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBartTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/WorkSpace/bishe/module.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn_conv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgcn_conv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRGCNConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaForMaskedLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhetero_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m from torch_geometric.data.storage import (BaseStorage, EdgeStorage,\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda_spec\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcpu_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         raise ImportError(f\"Could not find module '{library}_cpu' in \"\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/bishe/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so, 6): Symbol not found: __ZN2at8internal13_parallel_runExxxRKNSt3__18functionIFvxxmEEE\n  Referenced from: /usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so\n  Expected in: /usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch/lib/libtorch_cpu.dylib\n in /usr/local/anaconda3/envs/bishe/lib/python3.7/site-packages/torch_sparse/_convert_cpu.so"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch import nn\n",
    "from logging import getLogger\n",
    "from data import Vocab, NLP, S2SDataset\n",
    "from utils import build_optimizer, init_seed, init_logger, init_device, read_configuration, collate_fn_graph_text, \\\n",
    "    format_time\n",
    "from module import GraphEncoder, GraphReconstructor, GraphPointer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BertModel, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def compute_kd_loss(node_embeddings, desc_embeddings, node_masks, kd_masks):\n",
    "    assert node_embeddings.size() == desc_embeddings.size()\n",
    "    mse_loss = nn.MSELoss(reduction='none')\n",
    "    loss = mse_loss(node_embeddings, desc_embeddings)\n",
    "    loss = loss.mean(dim=-1)\n",
    "    masks = node_masks * kd_masks\n",
    "    loss = loss.masked_select(masks).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_ce_loss(logits, labels, masks):\n",
    "    ce_loss = nn.CrossEntropyLoss(ignore_index=0, reduction=\"none\")\n",
    "    loss = ce_loss(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    loss = loss.reshape_as(labels)\n",
    "    loss = loss.masked_select(masks).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def run_train_batch(config, batch, teacher, student, plm, reconstructor, copyer,\n",
    "                    plm_optimizer, external_optimizer, device):\n",
    "    nodes, edges, types, node_masks, kd_description, kd_description_masks, kd_positions, \\\n",
    "        recon_relations, recon_positions, recon_masks, gen_outputs, gen_masks, pointer, pointer_masks = batch\n",
    "\n",
    "    kd_description = kd_description.to(device)\n",
    "    kd_description_masks = kd_description_masks.to(device)\n",
    "    output_dict = teacher(input_ids=kd_description,\n",
    "                          attention_mask=kd_description_masks,\n",
    "                          output_hidden_states=True,\n",
    "                          return_dict=True)\n",
    "    positions = kd_positions.unsqueeze(-1).expand(-1, -1, output_dict[\"encoder_last_hidden_state\"].size(-1)).to(device)\n",
    "    teacher_embeddings = torch.gather(output_dict[\"encoder_last_hidden_state\"], dim=1, index=positions)\n",
    "    teacher_embeddings = teacher_embeddings.detach()\n",
    "\n",
    "    nodes = nodes.to(device)\n",
    "    student_embeddings = student(nodes, edges, types)\n",
    "\n",
    "    node_masks = node_masks.to(device)\n",
    "    kd_masks = torch.ne(kd_positions, 0).to(device)\n",
    "    kd_loss = compute_kd_loss(student_embeddings, teacher_embeddings, node_masks, kd_masks)\n",
    "\n",
    "    gen_outputs = gen_outputs.to(device)\n",
    "    gen_masks = gen_masks.to(device)\n",
    "    output_dict = plm(input_ids=None,\n",
    "                      inputs_embeds=teacher_embeddings,\n",
    "                      attention_mask=node_masks,\n",
    "                      decoder_input_ids=gen_outputs[:, :-1],\n",
    "                      decoder_attention_mask=gen_masks[:, :-1],\n",
    "                      output_hidden_states=True,\n",
    "                      labels=gen_outputs[:, 1:].contiguous(),\n",
    "                      return_dict=True)\n",
    "    gen_loss = output_dict[\"loss\"]\n",
    "\n",
    "    decoder_input_embeddings = plm.get_input_embeddings()(gen_outputs[:, :-1])\n",
    "    decoder_output_hiddens = output_dict[\"decoder_hidden_states\"][-1]\n",
    "    pointer = pointer.to(device)\n",
    "    pointer_masks = pointer_masks.to(device)\n",
    "    copy_prob = copyer(decoder_input_embeddings, decoder_output_hiddens, pointer[:, 1:])\n",
    "    copy_loss = copy_prob.masked_select(pointer_masks[:, 1:]).mean()\n",
    "\n",
    "    recon_positions = recon_positions.to(device)\n",
    "    recon_relations = recon_relations.to(device)\n",
    "    recon_masks = recon_masks.to(device)\n",
    "    rec_logits = reconstructor(recon_positions, output_dict[\"encoder_hidden_states\"][-1])\n",
    "    rec_loss = compute_ce_loss(rec_logits, recon_relations, recon_masks)\n",
    "\n",
    "    loss = gen_loss + rec_loss * config[\"rec_weight\"] + kd_loss * config[\"kd_weight\"] + copy_loss * config[\"cp_weight\"]\n",
    "\n",
    "    plm_optimizer.zero_grad()\n",
    "    external_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    external_optimizer.step()\n",
    "    plm_optimizer.step()\n",
    "\n",
    "    return gen_loss.item(), rec_loss.item(), kd_loss.item(), copy_loss.item()\n",
    "\n",
    "\n",
    "def run_eval_batch(config, batch, teacher, student, plm, reconstructor, copyer, device):\n",
    "    nodes, edges, types, node_masks, kd_description, kd_description_masks, kd_positions, \\\n",
    "        recon_relations, recon_positions, recon_masks, gen_outputs, gen_masks, pointer, pointer_masks = batch\n",
    "\n",
    "    kd_description = kd_description.to(device)\n",
    "    kd_description_masks = kd_description_masks.to(device)\n",
    "    output_dict = teacher(input_ids=kd_description,\n",
    "                          attention_mask=kd_description_masks,\n",
    "                          output_hidden_states=True,\n",
    "                          return_dict=True)\n",
    "    positions = kd_positions.unsqueeze(-1).expand(-1, -1, output_dict[\"encoder_last_hidden_state\"].size(-1)).to(device)\n",
    "    teacher_embeddings = torch.gather(output_dict[\"encoder_last_hidden_state\"], dim=1, index=positions)\n",
    "    teacher_embeddings = teacher_embeddings.detach()\n",
    "\n",
    "    nodes = nodes.to(device)\n",
    "    student_embeddings = student(nodes, edges, types)\n",
    "\n",
    "    node_masks = node_masks.to(device)\n",
    "    kd_masks = torch.ne(kd_positions, 0).to(device)\n",
    "    kd_loss = compute_kd_loss(student_embeddings, teacher_embeddings, node_masks, kd_masks)\n",
    "\n",
    "    gen_outputs = gen_outputs.to(device)\n",
    "    gen_masks = gen_masks.to(device)\n",
    "    output_dict = plm(input_ids=None,\n",
    "                      inputs_embeds=student_embeddings,\n",
    "                      attention_mask=node_masks,\n",
    "                      decoder_input_ids=gen_outputs[:, :-1],\n",
    "                      decoder_attention_mask=gen_masks[:, :-1],\n",
    "                      output_hidden_states=True,\n",
    "                      labels=gen_outputs[:, 1:].contiguous(),\n",
    "                      return_dict=True)\n",
    "    gen_loss = output_dict[\"loss\"]\n",
    "\n",
    "    decoder_input_embeddings = plm.get_input_embeddings()(gen_outputs[:, :-1])\n",
    "    decoder_output_hiddens = output_dict[\"decoder_hidden_states\"][-1]\n",
    "    pointer = pointer.to(device)\n",
    "    pointer_masks = pointer_masks.to(device)\n",
    "    copy_prob = copyer(decoder_input_embeddings, decoder_output_hiddens, pointer[:, 1:])\n",
    "    copy_loss = copy_prob.masked_select(pointer_masks[:, 1:]).mean()\n",
    "\n",
    "    recon_positions = recon_positions.to(device)\n",
    "    recon_relations = recon_relations.to(device)\n",
    "    recon_masks = recon_masks.to(device)\n",
    "    rec_logits = reconstructor(recon_positions, output_dict[\"encoder_hidden_states\"][-1])\n",
    "    rec_loss = compute_ce_loss(rec_logits, recon_relations, recon_masks)\n",
    "\n",
    "    return gen_loss.item(), rec_loss.item(), kd_loss.item(), copy_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config):\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "\n",
    "    logger.info(config)\n",
    "    init_seed(config[\"seed\"], config[\"reproducibility\"])\n",
    "    device = init_device(config)\n",
    "\n",
    "    logger.info(\"Build node and relation vocabularies.\")\n",
    "    vocabs = dict()\n",
    "    vocabs[\"node\"] = Vocab(config[\"node_vocab\"])\n",
    "    vocabs[\"relation\"] = Vocab(config[\"relation_vocab\"])\n",
    "\n",
    "    # logger.info(\"Build Teacher Model.\")\n",
    "    # teacher = BartForConditionalGeneration.from_pretrained(config[\"teacher_dir\"])\n",
    "    # teacher.requires_grad = False\n",
    "    # for para in teacher.parameters():\n",
    "    #     para.requires_grad = False\n",
    "    # teacher.to(device)\n",
    "\n",
    "    logger.info(\"Build Student Model.\")\n",
    "    student = GraphEncoder(vocabs[\"node\"].size(), vocabs[\"relation\"].size(),\n",
    "                           config[\"gnn_layers\"], config[\"embedding_size\"], config[\"node_embedding\"])\n",
    "    student.load_state_dict(torch.load(config[\"external_model\"])[\"student\"])\n",
    "    student.to(device)\n",
    "\n",
    "    logger.info(\"Build PLM Model.\")\n",
    "    bart_tokenizer = BartTokenizer.from_pretrained(config[\"fine_tuned_plm_dir\"])\n",
    "    plm = BartForConditionalGeneration.from_pretrained(config[\"fine_tuned_plm_dir\"])\n",
    "    plm.to(device)\n",
    "\n",
    "    logger.info(\"Create testing dataset.\")\n",
    "    test_dataloader = DataLoader(\n",
    "        S2SDataset(data_dir=config[\"data_dir\"], dataset=config[\"dataset\"],\n",
    "                   tokenizer=bart_tokenizer, node_vocab=vocabs[\"node\"], relation_vocab=vocabs[\"relation\"],\n",
    "                   num_samples=\"all\", usage=\"test\"),\n",
    "        batch_size=config[\"test_batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        drop_last=False,\n",
    "        collate_fn=collate_fn_graph_text,\n",
    "        pin_memory=True)\n",
    "\n",
    "    student.eval()\n",
    "    # teacher.eval()\n",
    "    plm.eval()\n",
    "    idx = 0\n",
    "    generated_text = []\n",
    "    reference_text = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            nodes, edges, types, node_masks, kd_description, kd_description_masks, kd_positions, \\\n",
    "                recon_relations, recon_positions, recon_masks, gen_outputs, gen_masks, pointer, pointer_masks = batch\n",
    "\n",
    "            # kd_description = kd_description.to(device)\n",
    "            # kd_description_masks = kd_description_masks.to(device)\n",
    "            # output_dict = teacher(kd_description,\n",
    "            #                       attention_mask=kd_description_masks,\n",
    "            #                       output_hidden_states=True,\n",
    "            #                       return_dict=True)\n",
    "            # positions = kd_positions.unsqueeze(-1).expand(-1, -1, output_dict[\"encoder_last_hidden_state\"].size(-1)).to(device)\n",
    "            # teacher_embeddings = torch.gather(output_dict[\"encoder_last_hidden_state\"], dim=1, index=positions).detach()\n",
    "\n",
    "            nodes = nodes.to(device)\n",
    "            student_embeddings = student(nodes, edges, types)\n",
    "\n",
    "            node_masks = node_masks.to(device)\n",
    "            generated_ids = plm.generate(input_ids=None,\n",
    "                                         inputs_embeds=student_embeddings,\n",
    "                                         attention_mask=node_masks,\n",
    "                                         num_beams=4,\n",
    "                                         max_length=config[\"max_seq_length\"],\n",
    "                                         early_stopping=True)\n",
    "\n",
    "            generated = bart_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            reference = bart_tokenizer.batch_decode(gen_outputs, skip_special_tokens=True)\n",
    "            generated_text.extend(generated)\n",
    "            reference_text.extend(reference)\n",
    "\n",
    "            idx += 1\n",
    "            logger.info(\"Finish {}-th example.\".format(idx))\n",
    "\n",
    "    assert len(generated_text) == len(reference_text)\n",
    "    saved_file = \"{}-{}.res\".format(config[\"dataset\"], config[\"num_samples\"])\n",
    "    saved_file_path = os.path.join(config[\"output_dir\"], saved_file)\n",
    "    fout = open(saved_file_path, \"w\")\n",
    "    for i in range(len(generated_text)):\n",
    "        fout.write(\"Generated text: \" + generated_text[i].strip() + \"\\n\")\n",
    "        fout.write(\"Reference text: \" + reference_text[i].strip() + \"\\n\")\n",
    "    fout.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
