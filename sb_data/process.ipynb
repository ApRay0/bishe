{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54040a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import unidecode\n",
    "import random\n",
    "import spacy\n",
    "from transformers import RobertaTokenizer, BartTokenizer, BertTokenizer\n",
    "\n",
    "\n",
    "class NLP:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'tagger'])\n",
    "        self.nlp.add_pipe('sentencizer')\n",
    "\n",
    "    def sent_tokenize(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        sentences = [sent.string.strip() for sent in doc.sents]\n",
    "        return sentences\n",
    "\n",
    "    def word_tokenize(self, text, lower=False):  # create a tokenizer function\n",
    "        if text is None:\n",
    "            return text\n",
    "        text = ' '.join(text.split())\n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "        toks = [tok.text for tok in self.nlp.tokenizer(text)]\n",
    "        return ' '.join(toks)\n",
    "\n",
    "\n",
    "nlp = NLP()\n",
    "\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    d = [m.group(0) for m in matches]\n",
    "    new_d = []\n",
    "    for token in d:\n",
    "        token = token.replace('(', '')\n",
    "        token_split = token.split('_')\n",
    "        for t in token_split:\n",
    "            new_d.append(t)\n",
    "    new_d = \" \".join(new_d)\n",
    "    return new_d\n",
    "\n",
    "\n",
    "def get_nodes(n):\n",
    "    n = unidecode.unidecode(n.strip().lower())\n",
    "    n = n.replace('-', ' ')\n",
    "    n = n.replace('_', ' ')\n",
    "    n = nlp.word_tokenize(n)\n",
    "\n",
    "    return n\n",
    "\n",
    "\n",
    "def get_relation(n):\n",
    "    n = unidecode.unidecode(n.strip().lower())\n",
    "    n = n.replace('-', ' ')\n",
    "    n = n.replace('_', ' ')\n",
    "    n = nlp.word_tokenize(n)\n",
    "\n",
    "    return n\n",
    "\n",
    "\n",
    "def get_text(txt, lower=True):\n",
    "    if lower:\n",
    "        txt = txt.lower()\n",
    "    txt = unidecode.unidecode(txt.strip())\n",
    "    txt = txt.replace('-', ' ')\n",
    "    txt = nlp.word_tokenize(txt)\n",
    "\n",
    "    return txt\n",
    "\n",
    "\n",
    "def BFS(graph, s):\n",
    "    queue = [s]\n",
    "    seen = [s]\n",
    "    node_seq = []\n",
    "    while queue:\n",
    "        vertex = queue.pop(0)\n",
    "        adj_nodes = graph[vertex]\n",
    "        for w in adj_nodes:\n",
    "            if w not in seen:\n",
    "                queue.append(w)\n",
    "                seen.append(w)\n",
    "        node_seq.append(vertex)\n",
    "    return node_seq\n",
    "\n",
    "\n",
    "def DFS(graph, s):\n",
    "    stack = [s]\n",
    "    seen = [s]\n",
    "    node_seq = []\n",
    "    while stack:\n",
    "        vertex = stack.pop()\n",
    "        adj_nodes = graph[vertex]\n",
    "        for w in adj_nodes:\n",
    "            if w not in seen:\n",
    "                stack.append(w)\n",
    "                seen.append(w)\n",
    "        node_seq.append(vertex)\n",
    "    return node_seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebd66308",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BartTokenizer.from_pretrained('../pretrained_model/bart-large')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('../pretrained_model/bart-large')\n",
    "\n",
    "# print(tokenizer.decoder_start_token_id)\n",
    "# exit(0)\n",
    "\n",
    "filename = ['./sb-process-2/general.json']\n",
    "\n",
    "for fn in filename:\n",
    "    fin = open(fn, \"r\")\n",
    "    data = json.load(fin)\n",
    "    fin.close()\n",
    "\n",
    "    fout = open(fn[:-5] + \"_processed.json\", \"w\")\n",
    "    for d in data:\n",
    "        new_dict = dict()\n",
    "\n",
    "        # -------WebNLG dataset------\n",
    "        valid = True\n",
    "        ner_dict = {}\n",
    "        ren_dict = {}\n",
    "        for k, v in d['ner2ent'].items():\n",
    "            en = get_nodes(v)\n",
    "            if en == \"\":\n",
    "                valid = False\n",
    "            ner_dict[k] = en\n",
    "            ren_dict[en] = k\n",
    "        new_dict['ner2ent'] = ner_dict\n",
    "        new_dict['ent2ner'] = ren_dict\n",
    "        # -------WebNLG dataset------\n",
    "\n",
    "        # -------Agenda dataset------\n",
    "        # new_dict['title'] = get_text(d['title'])\n",
    "        # types = d['types'].split()\n",
    "        # valid = True\n",
    "        # ner_dict = {}\n",
    "        # ren_dict = {}\n",
    "        # for idx in range(len(types)):\n",
    "        #     en = get_nodes(d['entities'][idx])\n",
    "        #     if en == \"\":\n",
    "        #         valid = False\n",
    "        #     ner = types[idx][1:-1].upper()\n",
    "        #     ner_dict[ner + \"_\" + str(idx)] = en\n",
    "        #     ren_dict[en] = ner + \"_\" + str(idx)\n",
    "        # new_dict['ner2ent'] = ner_dict\n",
    "        # new_dict['ent2ner'] = ren_dict\n",
    "        # -------WebNLG dataset------\n",
    "\n",
    "        # -------Genwiki dataset------\n",
    "        # valid = True\n",
    "        # ner_dict = {}\n",
    "        # ren_dict = {}\n",
    "        # for idx, ent in enumerate(d['entities']):\n",
    "        #     ner = \"ENT_\" + str(idx)\n",
    "        #     en = get_nodes(ent)\n",
    "        #     if en == \"\":\n",
    "        #         valid = False\n",
    "        #     ner_dict[ner] = en\n",
    "        #     ren_dict[en] = ner\n",
    "        # new_dict['ner2ent'] = ner_dict\n",
    "        # new_dict['ent2ner'] = ren_dict\n",
    "        # -------WebNLG dataset------\n",
    "\n",
    "        if not valid:\n",
    "            continue\n",
    "\n",
    "        temp = []\n",
    "        serialization = []\n",
    "        for tri in d['triples']:\n",
    "            h = get_nodes(tri[0])\n",
    "            t = get_nodes(tri[2])\n",
    "            r = camel_case_split(get_relation(tri[1]))\n",
    "            new_t = [h, r, t]\n",
    "            temp.append(new_t)\n",
    "            serialization.extend([\"<Head>\", h, \"<Relation>\", r, \"<Tail>\", t])\n",
    "        new_dict['triples'] = temp\n",
    "        new_dict['triples_serialization'] = serialization\n",
    "\n",
    "        tokens = []\n",
    "        for token in d['target'].split():\n",
    "            if token.isupper() and '_' in token:\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.append(token.lower())\n",
    "        new_dict['target'] = get_text(' '.join(tokens), lower=False)\n",
    "\n",
    "        try:\n",
    "            tokens = []\n",
    "            nodes = []\n",
    "            for token in new_dict['target'].split():\n",
    "                if token.isupper():\n",
    "                    tokens.append(new_dict['ner2ent'][token])\n",
    "                    if new_dict['ner2ent'][token] not in nodes:\n",
    "                        nodes.append(new_dict['ner2ent'][token])\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "            new_dict['target_txt'] = (' '.join(tokens)).lower()\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        new_dict['plm_output'] = bart_tokenizer.tokenize(new_dict['target_txt'])\n",
    "\n",
    "        test_output = []\n",
    "        pointer = []\n",
    "        idx = 0\n",
    "        for tok in new_dict['target'].split():\n",
    "            if idx == 0:\n",
    "                if tok.isupper():\n",
    "                    ent = bart_tokenizer.tokenize(new_dict['ner2ent'][tok])\n",
    "                    test_output.extend(ent)\n",
    "                    pointer.extend([1] * len(ent))\n",
    "                else:\n",
    "                    word = bart_tokenizer.tokenize(tok)\n",
    "                    test_output.extend(word)\n",
    "                    pointer.extend([0] * len(word))\n",
    "            else:\n",
    "                if tok.isupper():\n",
    "                    ent = bart_tokenizer.tokenize(\" \" + new_dict['ner2ent'][tok])\n",
    "                    test_output.extend(ent)\n",
    "                    pointer.extend([1] * len(ent))\n",
    "                else:\n",
    "                    word = bart_tokenizer.tokenize(\" \" + tok)\n",
    "                    test_output.extend(word)\n",
    "                    pointer.extend([0] * len(word))\n",
    "            idx += 1\n",
    "\n",
    "        assert len(pointer) == len(new_dict['plm_output']), \"The length of pointer and output are not equal!\"\n",
    "        assert test_output == new_dict['plm_output'], \"The test output and plm output are not equal!\"\n",
    "\n",
    "        new_dict['pointer'] = pointer\n",
    "\n",
    "        adject = dict()\n",
    "        for t in new_dict['triples']:\n",
    "            if t[0] not in nodes:\n",
    "                nodes.append(t[0])\n",
    "            if t[2] not in nodes:\n",
    "                nodes.append(t[2])\n",
    "\n",
    "            if t[0] not in adject:\n",
    "                adject[t[0]] = []\n",
    "            adject[t[0]].append(t[2])\n",
    "            if t[2] not in adject:\n",
    "                adject[t[2]] = []\n",
    "            adject[t[2]].append(t[0])\n",
    "\n",
    "\n",
    "        new_dict['nodes'] = nodes\n",
    "\n",
    "        edges = [[], []]\n",
    "        types = []\n",
    "        for t in new_dict['triples']:\n",
    "            hid = new_dict['nodes'].index(t[0])\n",
    "            tid = new_dict['nodes'].index(t[2])\n",
    "            edges[0].append(hid)\n",
    "            edges[1].append(tid)\n",
    "            types.append(t[1])\n",
    "            edges[1].append(hid)\n",
    "            edges[0].append(tid)\n",
    "            types.append(t[1])\n",
    "        new_dict['edges'] = edges\n",
    "        new_dict['types'] = types\n",
    "\n",
    "        word_nodes = [bert_tokenizer.tokenize(node) for node in new_dict['nodes']]\n",
    "        new_dict['split_nodes'] = [nd for nodes in word_nodes for nd in nodes]\n",
    "\n",
    "        start = 0\n",
    "        split2start = {}\n",
    "        for idx in range(len(word_nodes)):\n",
    "            split2start[idx] = start\n",
    "            start += len(word_nodes[idx])\n",
    "\n",
    "        split_edges = [[], []]\n",
    "        split_types = []\n",
    "        pairs = []\n",
    "        relations = []\n",
    "        for tri in new_dict['triples']:\n",
    "            h, r, t = bert_tokenizer.tokenize(tri[0]), tri[1], bert_tokenizer.tokenize(tri[2])\n",
    "            hidx = word_nodes.index(h)\n",
    "            tidx = word_nodes.index(t)\n",
    "            pairs.append([[split2start[hidx], split2start[hidx] + len(h) - 1],\n",
    "                          [split2start[tidx], split2start[tidx] + len(t) - 1]])\n",
    "            relations.append(r)\n",
    "            for i, hn in enumerate(word_nodes[hidx]):\n",
    "                for j, tn in enumerate(word_nodes[tidx]):\n",
    "                    split_edges[0].append(split2start[hidx] + i)\n",
    "                    split_edges[1].append(split2start[tidx] + j)\n",
    "                    split_types.append(r)\n",
    "                    split_edges[1].append(split2start[hidx] + i)\n",
    "                    split_edges[0].append(split2start[tidx] + j)\n",
    "                    split_types.append(r)\n",
    "        new_dict['split_edges'] = split_edges\n",
    "        new_dict['split_types'] = split_types\n",
    "        new_dict['pairs'] = pairs\n",
    "        new_dict['relations'] = relations\n",
    "\n",
    "        assert len(new_dict['pairs']) == len(new_dict['relations']), \"the length of pairs and relations are not equal\"\n",
    "\n",
    "        target_tokens = new_dict['target'].split()\n",
    "\n",
    "        order2ent = {}\n",
    "        used_ner = set()\n",
    "        new_target_tokens = []\n",
    "        order = 1\n",
    "        for idx, token in enumerate(target_tokens):\n",
    "            if token.isupper():\n",
    "                if token not in used_ner:\n",
    "                    new_target_tokens.append('<mask>')\n",
    "                    ent = new_dict['ner2ent'][token]\n",
    "                    used_ner.add(token)\n",
    "                    order2ent[order] = ent\n",
    "                    order += 1\n",
    "                else:\n",
    "                    ent = new_dict['ner2ent'][token]\n",
    "                    new_target_tokens.append(ent)\n",
    "            else:\n",
    "                new_target_tokens.append(token)\n",
    "\n",
    "        target_tokens = [\"<s>\"] + bert_tokenizer.tokenize(' '.join(new_target_tokens)) + [\"</s>\"]\n",
    "\n",
    "        positions = [[0] * len(bert_tokenizer.tokenize(ent)) for ent in new_dict['nodes']]\n",
    "        masked_target_tokens = []\n",
    "        new_target_tokens = []\n",
    "        order = 1\n",
    "        for idx, token in enumerate(target_tokens):\n",
    "            if token == '<mask>':\n",
    "                ent = order2ent[order]\n",
    "                ent_len = len(bert_tokenizer.tokenize(ent))\n",
    "                start = len(masked_target_tokens)\n",
    "                ent_idx = new_dict['nodes'].index(ent)\n",
    "                positions[ent_idx] = list(range(start, start + ent_len))\n",
    "                masked_target_tokens.extend(['<mask>'] * ent_len)\n",
    "                new_target_tokens.extend(bert_tokenizer.tokenize(ent))\n",
    "                order += 1\n",
    "            else:\n",
    "                masked_target_tokens.append(token)\n",
    "                new_target_tokens.append(token)\n",
    "        positions = [p for pos in positions for p in pos]\n",
    "\n",
    "        new_dict['positions'] = positions\n",
    "        new_dict['description'] = new_target_tokens\n",
    "        new_dict['masked_description'] = masked_target_tokens\n",
    "\n",
    "        assert len(new_dict['split_nodes']) == len(new_dict['positions'])\n",
    "\n",
    "        fout.write(json.dumps(new_dict, ensure_ascii=False) + \"\\n\")\n",
    "    fout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "221c99a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../pretrained_model/bart-large/tokenizer_config.json',\n",
       " '../pretrained_model/bart-large/special_tokens_map.json',\n",
       " '../pretrained_model/bart-large/vocab.json',\n",
       " '../pretrained_model/bart-large/merges.txt',\n",
       " '../pretrained_model/bart-large/added_tokens.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "tokenizer.save_pretrained(\"../pretrained_model/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dba881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
